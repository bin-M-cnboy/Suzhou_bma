{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0340b265",
   "metadata": {},
   "source": [
    "# 1. 索引构建组件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6529ccc1",
   "metadata": {},
   "source": [
    "### 1. extract_triples_eng_DEMO.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b936a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "extract_triples_eng_DEMO.py\n",
    "\n",
    "数据集.json_文件 ->\n",
    "取出answer,question,context -> answer_list,question_list,docs_list\n",
    "doc -> 长文本sentence -> 输入张量model_inputs \n",
    "(仅用id ??? ) -> 输入张量的id列表 ids_list -> 对齐的id张量 ids_tensor\n",
    "-> 使用Qwen_LLM -> generated_ids -> 解码为知识图谱三元组 response[]\n",
    "人工整理为文件 -> 三元组文本行_文件\n",
    "'''\n",
    "\n",
    "device = \"cuda:0\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen1.5-32B-Chat-AWQ\",\n",
    "    device_map=device\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen1.5-32B-Chat-AWQ\")\n",
    "\n",
    "path = './src/dataset/HOTPOPQA'\n",
    "\n",
    "answer_list = []\n",
    "question_list = []\n",
    "docs_list = []\n",
    "cnt = 0\n",
    "\n",
    "with open(f'{path}/hotpot_dev_fullwiki_v1.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        answer = data[i]['answer']\n",
    "        question = data[i]['question']\n",
    "        docs = data[i]['context']\n",
    "\n",
    "        tmpd = []\n",
    "        for j in range(len(docs)):\n",
    "            tmpd.append(docs[j][1])\n",
    "\n",
    "        answer_list.append(answer)\n",
    "        question_list.append(question)\n",
    "        docs_list.append(tmpd)\n",
    "\n",
    "\n",
    "for docs in docs_list:\n",
    "    ids_list = []\n",
    "    cnt += 1\n",
    "    if cnt > 186:   # 只处理第186条以后的数据\n",
    "        for i in range(len(docs)):# 打包输入\n",
    "            sentence = ''.join(docs[i][j] for j in range(len(docs[i])))# 将文档中的句子连接成完整文本\n",
    "\n",
    "            messages = [{\"role\": \"system\",\n",
    "                              \"content\": \"You are an NLP assistant. Given a piece of text, you need to analyze its semantic information and generate a knowledge graph. Your output consists only of triples, considering only the text content, and avoiding newline characters. For example, (entity; relationship; entity),(entity; relationship; entity). The knowledge graph should be comprehensive, covering all information in the text.\"},\n",
    "                        {\"role\": \"user\",\n",
    "                              \"content\": 'Adam Collis is an American filmmaker and actor. He attended the Duke University from 1986 to 1990 and the University of California, Los Angeles from 2007 to 2010. He also studied cinema at the University of Southern California from 1991 to 1997. Collis first work was the assistant director for the Scott Derrickson\\'s short \"Love in the Ruins\" (1995). In 1998, he played \"Crankshaft\" in Eric Koyanagi\\'s \"Hundred Percent\".'},\n",
    "                                  {\"role\": \"assistant\",\n",
    "                              \"content\": '(Adam Collis; nationality; American),(Adam Collis; profession; filmmaker),(Adam Collis; profession; actor),(Adam Collis; education; Duke University),(Adam Collis; education; University of California, Los Angeles),(Adam Collis; education; University of Southern California),(Adam Collis; attended; Duke University),(Adam Collis; attended; University of California, Los Angeles),(Adam Collis; attended; University of Southern California),(Adam Collis; first work; assistant director),(Adam Collis; work; \"Love in the Ruins\"),(\"Love in the Ruins\"; director; Scott Derrickson),(Adam Collis; work date; 1995),(Adam Collis; role; \"Crankshaft\"),(\"Hundred Percent\"; director; Eric Koyanagi),(Adam Collis; work; \"Hundred Percent\"),(Adam Collis; work date; 1998)'},\n",
    "                        {\"role\": \"user\",\n",
    "                         \"content\": 'Tyler Bates (born June 5, 1965) is an American musician, music producer, and composer for films, television, and video games. Much of his work is in the action and horror film genres, with films like \"Dawn of the Dead, 300, Sucker Punch,\" and \"John Wick.\" He has collaborated with directors like Zack Snyder, Rob Zombie, Neil Marshall, William Friedkin, Scott Derrickson, and James Gunn. With Gunn, he has scored every one of the director\\'s films; including \"Guardians of the Galaxy\", which became one of the highest grossing domestic movies of 2014, and its 2017 sequel. In addition, he is also the lead guitarist of the American rock band Marilyn Manson, and produced its albums \"The Pale Emperor\" and \"Heaven Upside Down\".'},\n",
    "                        {\"role\": \"assistant\",\n",
    "                         \"content\": '(Tyler Bates; birthdate; June 5, 1965),(Tyler Bates; nationality; American),(Tyler Bates; profession; musician),(Tyler Bates; profession; music producer),(Tyler Bates; profession; composer),(Tyler Bates; works in; films),(Tyler Bates; works in; television),(Tyler Bates; works in; video games),(Tyler Bates; specializes in; action and horror film genres),(Tyler Bates; notable films; \"Dawn of the Dead\"),(Tyler Bates; notable films; \"300\"),(Tyler Bates; notable films; \"Sucker Punch\"),(Tyler Bates; notable films; \"John Wick\"),(Tyler Bates; collaborations; Zack Snyder),(Tyler Bates; collaborations; Rob Zombie),(Tyler Bates; collaborations; Neil Marshall),(Tyler Bates; collaborations; William Friedkin),(Tyler Bates; collaborations; Scott Derrickson),(Tyler Bates; collaborations; James Gunn),(Tyler Bates; collaborations; James Gunn),(Tyler Bates; collaborations; James Gunn),(Tyler Bates; scored; \"Guardians of the Galaxy\"),(\"Guardians of the Galaxy\"; release year; 2014),(\"Guardians of the Galaxy\"; grossing; high),(\"Guardians of the Galaxy\"; sequel; released in 2017),(Tyler Bates; lead guitarist; Marilyn Manson),(Marilyn Manson; music albums; \"The Pale Emperor\"),(Marilyn Manson; music albums; \"Heaven Upside Down\")'}\n",
    "                        ]\n",
    "            messages.append({\"role\": \"user\",\n",
    "                            \"content\": sentence})\n",
    "\n",
    "            text = tokenizer.apply_chat_template(  # 将messages输入\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True  # 添加生成提示\n",
    "            )\n",
    "            # 将文本转换为模型输入张量\n",
    "            model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "            ids_list.append(model_inputs.input_ids.squeeze())   # 存储ID序列\n",
    "        \n",
    "        # 对所有id进行padding填充对齐\n",
    "        max_length = max([x.shape[0] for x in ids_list])\n",
    "        pad_id = tokenizer.pad_token_id     # 获取需填充的ID\n",
    "        for i in range(len(ids_list)):\n",
    "            pad_length = max_length - ids_list[i].shape[0]\n",
    "            ids_list[i] = torch.concat([torch.tensor(pad_id, device=device).repeat(pad_length), ids_list[i]], 0)\n",
    "\n",
    "        ids_tensor = torch.stack(ids_list) # 将ids_list列表转换为张量ids_tensor\n",
    "\n",
    "        # 生成知识图谱三元组\n",
    "        generated_ids = model.generate( # 输入ids_tensor, model输出结果generated_ids\n",
    "            ids_tensor,\n",
    "            max_new_tokens=2048 # 最大新生成的token数量\n",
    "        )\n",
    "        generated_ids = [   # generated_ids = [len(input_ids) -> output_ids] -> 使仅包含新生成部分的ID\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(ids_tensor, generated_ids)\n",
    "        ]\n",
    "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)# 将生成的ID转换为文本\n",
    "\n",
    "        for i in range(len(response)):\n",
    "            print(response[i])\n",
    "        print('------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b5f933",
   "metadata": {},
   "source": [
    "### 2. src/dataset/preprocess/preprocess_hotpop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a96a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "preprocess_hotpop.py\n",
    "\n",
    "'{path}/outputs(500).txt'三元组 -> 知识图谱_文件/图张量_文件      # 得到 实体级层\n",
    "数据集_文件 -> 知识图谱_文件/问题张量_文件, 文档张量_文件           # 得到 文档级层\n",
    "'''\n",
    "\n",
    "path = '../HOTPOPQA'\n",
    "\n",
    "nodeslist = []\n",
    "edgeslist = []\n",
    "deslist = []\n",
    "\n",
    "'''\n",
    "[1] 三元组文本行编码: 三元组文本行_文件 -> nodeslist,edgeslist\n",
    "\n",
    "三元组文本行_文件 -> step_one(): \n",
    "for file: textualize_graph(i,line)\n",
    "-> 三元组\n",
    "分批存入 -> nodeslist,edgeslist, des_文件\n",
    "'''\n",
    "def textualize_graph(i,line):# 从文本行中提取三元组并构建图结构\n",
    "    # 查找所有三元组(src;rel;dst) -> triples\n",
    "    triples = re.findall(r'\\((.*?)\\)', line)\n",
    "    nodes = {}\n",
    "    edges = []\n",
    "    des = {}\n",
    "    # triples -> nodes,edges,des\n",
    "    for tri in triples:\n",
    "        tri = tri.split(';') # 按分号分割三元组\n",
    "        if len(tri) != 3:# 移除错误的三元组（长度不为3）\n",
    "            if len(tri) >= 3:\n",
    "                tri[2] = ''.join(tri[2:len(tri)]) # 如果长度大于3，将多余部分合并到第三个元素\n",
    "            if len(tri) < 3:\n",
    "                continue # 如果长度小于3，跳过此三元组\n",
    "        \n",
    "        src, edge_attr, dst = tri[0].strip(),tri[1].strip(),tri[2].strip()# 提取src,rel,dst\n",
    "        # 添加新节点scr,dst -> nodes{}\n",
    "        if src not in nodes:\n",
    "            nodes[src] = len(nodes)     # node值 = 加入次序, node位置 = src值\n",
    "        if dst not in nodes:\n",
    "            nodes[dst] = len(nodes)\n",
    "        # 添加边信息 -> edges[]          # edge值 = {三元组}\n",
    "        edges.append({'src': nodes[src], 'edge_attr': edge_attr.strip(), 'dst': nodes[dst], })\n",
    "        # 添加描述 -> des{}\n",
    "        # 描述结构：des[src_id,dst_id]=(src,rel,dst) \n",
    "        des[str(nodes[src])+','+ str(nodes[dst])] = '(' + src +',' + edge_attr + ',' + dst + ')'\n",
    "\n",
    "    # 将节点和边信息转换为 Pandas DataFrame 格式\n",
    "    nodes = pd.DataFrame(nodes.items(), columns=['node_attr', 'node_id'])\n",
    "    edges = pd.DataFrame(edges)\n",
    "    return nodes,edges,des\n",
    "\n",
    "# 生成知识图谱和描述文件\n",
    "def step_one():\n",
    "    # 创建用于存储描述文件的目录\n",
    "    os.makedirs(f'{path}/KG_QA_vRobert/des500_new', exist_ok=True)\n",
    "    cnt = 1 # 计数器，用于des文件命名\n",
    "    tmp_nodeslist = [] \n",
    "    tmp_edgeslist = [] \n",
    "    tmp_deslist = [] \n",
    "    # file处理\n",
    "    with open(f'{path}/outputs(500).txt', 'r') as file:\n",
    "        for index, line in enumerate(file):\n",
    "                # 如果不是分隔符行\n",
    "                if line != '------------------------------------------\\n':\n",
    "                    # 提取图结构和描述\n",
    "                    nodes, edges,des = textualize_graph(index,line)\n",
    "                    tmp_nodeslist.append(nodes)\n",
    "                    tmp_edgeslist.append(edges)\n",
    "                    tmp_deslist.append(des)\n",
    "                else:\n",
    "                    # 遇到分隔符，将临时列表添加到全局列表\n",
    "                    nodeslist.append(tmp_nodeslist)\n",
    "                    edgeslist.append(tmp_edgeslist)\n",
    "                    # 将描述信息保存为 pickle 文件\n",
    "                    with open(f'{path}/KG_QA_vRobert/des500_new/{cnt}.pkl', 'wb') as file:\n",
    "                        pickle.dump(tmp_deslist, file)\n",
    "                    cnt += 1 # 计数器递增\n",
    "                    # 重置临时列表\n",
    "                    tmp_nodeslist = []\n",
    "                    tmp_edgeslist = []\n",
    "                    tmp_deslist = []\n",
    "\n",
    "'''\n",
    "[2] 编码知识图谱: nodeslist,edgeslist -> 知识图谱_文件/图张量_文件\n",
    "\n",
    "nodeslist,edgeslist -> step_two():\n",
    "取出nodes,edges -.node_attr列名> node_des[],edge_des[]\n",
    "-> 使用roberta_LLM:\n",
    "    node_des -> 张量x\n",
    "    edge_des -> 张量e\n",
    "torch: [edges.src, edges.dst] -> 索引张量edge_index\n",
    "-> data(节点特征x, 边索引edge_index, 边特征edge_attr=e, 节点数量num_nodes)\n",
    "存入-> 知识图谱_文件/图张量_文件/{i}/{j}\n",
    "'''\n",
    "def step_two():\n",
    "    model = SentenceTransformer('../../../all-roberta-large-v1',device='cuda:1')\n",
    "\n",
    "    # 创建用于存储知识图谱和图数据的目录\n",
    "    os.makedirs(f'{path}/KG_QA_vRobert', exist_ok=True)\n",
    "    os.makedirs(f'{path}/KG_QA_vRobert/graph500_new', exist_ok=True)\n",
    "\n",
    "    # 遍历nodeslist\n",
    "    for i in range(len(nodeslist)):\n",
    "        # 为每个知识图谱创建子目录\n",
    "        os.makedirs(f'{path}/KG_QA_vRobert/graph500_new/{i}', exist_ok=True)\n",
    "        for j in range(len(nodeslist[i])):  # 取出nodes,edges\n",
    "            nodes = nodeslist[i][j]\n",
    "            edges = edgeslist[i][j]\n",
    "            if not len(nodes) or not len(edges):# 如果节点或边为空, 跳过\n",
    "                continue\n",
    "\n",
    "            # 提取节点和边的描述\n",
    "            node_des = nodes.node_attr.tolist()\n",
    "            edge_des = edges.edge_attr.tolist()\n",
    "\n",
    "            # 使用模型编码节点和边的描述，并转换为 PyTorch 张量\n",
    "            x = model.encode(node_des,convert_to_tensor = True)\n",
    "            e = model.encode(edge_des,convert_to_tensor = True)\n",
    "            # 构建边的索引张量\n",
    "            edge_index = torch.LongTensor([edges.src, edges.dst])\n",
    "\n",
    "            # 创建 PyTorch Geometric 的 Data 对象，包含节点特征、边索引、边特征和节点数量\n",
    "            data = Data(x=x, edge_index=edge_index, edge_attr=e, num_nodes=len(nodes))\n",
    "\n",
    "            # 保存图数据到文件\n",
    "            torch.save(data, f'{path}/KG_QA_vRobert/graph500_new/{i}/{j}.pt')\n",
    "\n",
    "'''\n",
    "[3] 编码问题: 数据集_文件.question -> 问题张量_文件\n",
    "\n",
    "数据集_文件 -> encode_question()\n",
    "取出question\n",
    "-> 使用roberta_LLM:\n",
    "    question -> 张量question_emb\n",
    "存入 -> 问题张量_文件/{i}\n",
    "'''\n",
    "def encode_question():\n",
    "    model = SentenceTransformer('../../../all-roberta-large-v1')\n",
    "    # 创建用于存储问题嵌入的目录\n",
    "    os.makedirs(f'{path}/questions_emb_vRobert', exist_ok=True)\n",
    "\n",
    "    # 定义 HOTPOPQA 开发集的数据路径\n",
    "    data_path = '../HOTPOPQA/hotpot_dev_fullwiki_v1.json'\n",
    "    # 加载 JSON 格式的数据\n",
    "    with open(data_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # 遍历数据集中的每个问题\n",
    "    for i in range(len(data)):\n",
    "        question = data[i][\"question\"] # 提取问题文本\n",
    "        # 编码问题文本并转换为 PyTorch 张量\n",
    "        question_emb = model.encode(question, convert_to_tensor=True)\n",
    "        # 保存问题嵌入到文件\n",
    "        torch.save(question_emb, f'{path}/questions_emb_vRobert/{i}.pt')\n",
    "\n",
    "'''\n",
    "[3] 编码文档: 数据集_文件.context -> 知识图谱_文件/文档张量_文件\n",
    "\n",
    "数据集_文件 -> encode_doc()\n",
    "取出context -> docs\n",
    "取前500个数据项 -> 使用roberta_LLM:\n",
    "                    拼合过的passage -> 张量doc_emb\n",
    "存入 ->\n",
    "doc_emb -> doc_emb_list -> 知识图谱_文件/文档张量_文件/{500}\n",
    "passage -> doc_text_list -> 知识图谱_文件/文档列表_二进制文件/{500}\n",
    "\n",
    "'''\n",
    "def encode_doc():\n",
    "    model = SentenceTransformer('../../../all-roberta-large-v1',device='cuda:1')\n",
    "\n",
    "    # 创建用于存储知识图谱、文档文本和文档嵌入的目录\n",
    "    os.makedirs(f'{path}/KG_QA_vRobert', exist_ok=True)\n",
    "    os.makedirs(f'{path}/KG_QA_vRobert/doc_text', exist_ok=True)\n",
    "    os.makedirs(f'{path}/KG_QA_vRobert/doc_emb', exist_ok=True)\n",
    "\n",
    "    # 定义 HOTPOPQA 开发集的数据路径\n",
    "    data_path = '../HOTPOPQA/hotpot_dev_fullwiki_v1.json'\n",
    "    # 加载 JSON 格式的数据\n",
    "    with open(data_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    doc_text_list = [] # 存储文档文本列表\n",
    "    doc_emb_list = [] # 存储文档嵌入列表\n",
    "\n",
    "    # 遍历前 500 个数据项（假设每个数据项包含多个文档）\n",
    "    for i in range(500):\n",
    "        docs = data[i][\"context\"] # 提取上下文文档\n",
    "        for doc in docs:\n",
    "            # 将文档中的段落拼接成一个完整的文本\n",
    "            passage = ''.join(doc[1][j] for j in range(len(doc[1])))\n",
    "            # 编码文档文本并转换为 PyTorch 张量\n",
    "            doc_emb = model.encode(passage, convert_to_tensor=True)\n",
    "            doc_emb_list.append(doc_emb)\n",
    "            doc_text_list.append(passage)\n",
    "\n",
    "    # 保存所有文档嵌入到一个文件中\n",
    "    torch.save(torch.stack(doc_emb_list), f'{path}/KG_QA_vRobert/doc_emb/{500}.pt')\n",
    "    # 保存所有文档文本到 pickle 文件中\n",
    "    with open(f'{path}/KG_QA_vRobert/doc_text/{500}.pkl', 'wb') as file:\n",
    "        pickle.dump(doc_text_list, file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    step_one() # 生成知识图谱和描述\n",
    "    print(\"--- step_one() DONE! ---\")\n",
    "    step_two() # 编码知识图谱\n",
    "    print(\"--- step_two() DONE! ---\")\n",
    "    encode_question() # 编码问题\n",
    "    print(\"--- encode_question() DONE! ---\")\n",
    "    encode_doc() # 编码文档\n",
    "    print(\"--- encode_doc() DONE! ---\")\n",
    "    print(\"--- preprocess_hotpop.py DONE! ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0dd07e",
   "metadata": {},
   "source": [
    "### 3. run_demo_EX_hotpop_v0.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb0d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "加载过程:\n",
    "模型，预处理数据，问题嵌入，知识图谱数据，文档嵌入\n",
    "计算过程:    # ???自己跟自己找相似\n",
    "v0.py --- retrieval_func_two_stage()\n",
    "v1.py --- retrieval_func_with_att()\n",
    "'''\n",
    "\n",
    "def main(args):\n",
    "    batch_size = 1\n",
    "    n = 0 # 起始处理的数据索引\n",
    "    break_p = XXX # 结束处理的数据索引，XXX是一个占位符，取前K条数据进行实验\n",
    "\n",
    "    '''\n",
    "    加载过程:\n",
    "    模型，预处理数据，问题嵌入，知识图谱数据，文档嵌入\n",
    "    '''\n",
    "    # 加载模型\n",
    "    args.llm_model_path = llm_model_path[args.llm_model_name]\n",
    "    model = load_model[args.model_name](args=args)              \n",
    "    # 加载数据\n",
    "    test_dataset_path = './src/dataset/HOTPOPQA/hotpot_dev_fullwiki_v1.json'\n",
    "    test_dataset = load_dataset['HOTPOPQA'](test_dataset_path,size1=n,size2=break_p)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True, pin_memory=True, shuffle=False)\n",
    "\n",
    "    # 加载问题嵌入\n",
    "    questions_emb_list = []\n",
    "    for i in range(break_p):\n",
    "        question_emb = torch.load(f\"./src/dataset/HOTPOPQA/questions_emb_vRobert/{i}.pt\")\n",
    "        questions_emb_list.append(question_emb)\n",
    "\n",
    "    # 加载知识图谱数据\n",
    "    KG_lists = []\n",
    "    test_folder_path = \"./src/dataset/HOTPOPQA/KG_QA_vRobert\"\n",
    "    for i in range(n,break_p):\n",
    "        KG_path = f'{test_folder_path}/graph/{i}/'\n",
    "        if os.path.exists(KG_path):\n",
    "            for j in range(20):\n",
    "                if os.path.exists(KG_path + f'{j}.pt'):\n",
    "                    KG = torch.load(KG_path + f'{j}.pt')\n",
    "                    KG_lists.append(KG)\n",
    "                else:\n",
    "                    break\n",
    "        else: # 如果路径不存在，也尝试添加KG    # ???\n",
    "            KG_lists.append(KG)\n",
    "\n",
    "    # 加载文档嵌入张量\n",
    "    doc_embed_tensor = torch.load(f\"./src/dataset/HOTPOPQA/KG_QA_vRobert/doc_emb/{XXX}.pt\")\n",
    "    doc_embed_tensor = doc_embed_tensor[:len(KG_lists)] # 取前{len(KG_lists)}个\n",
    "\n",
    "    '''\n",
    "    计算过程    # ???自己跟自己找相似\n",
    "    '''\n",
    "    # 将文档嵌入作为子图列表张量\n",
    "    SG_lists_tensor = doc_embed_tensor\n",
    "    # 计算内积。内积越大，向量方向越接近\n",
    "    inner_product = torch.matmul(SG_lists_tensor, SG_lists_tensor.T)\n",
    "    # 计算L2范数。欧式距离\n",
    "    l2_norm = torch.norm(SG_lists_tensor, p=2, dim=1, keepdim=True)\n",
    "    # 归一化内积\n",
    "    n_prizes = inner_product / (l2_norm * l2_norm.T)\n",
    "    # 获取前3个最大值及索引(最相似的3个)\n",
    "    topk_n_values, topk_n_indices = torch.topk(n_prizes, 3, largest=True)\n",
    "    # 转置id，获取3个子图的边\n",
    "    SG_edges = topk_n_indices.T\n",
    "\n",
    "    # 加载描述信息des[]\n",
    "    des = []\n",
    "    for i in range(n,break_p):\n",
    "        des_path = f'{test_folder_path}/des/' + f'{i+1}.pkl'\n",
    "        f = open(des_path, 'rb')\n",
    "        tmp_des = pickle.load(f)\n",
    "        for j in range(len(tmp_des)):\n",
    "            des.append(tmp_des[j])\n",
    "\n",
    "    '''\n",
    "    评估阶段\n",
    "    v0.py --- retrieval_func_two_stage()\n",
    "    v1.py --- retrieval_func_with_att()\n",
    "    '''\n",
    "    predictions = [] # 存储模型预测结果\n",
    "    labels = [] # 存储真实标签\n",
    "\n",
    "    current_time = datetime.datetime.now()\n",
    "    print('Times:', current_time) # 打印当前时间\n",
    "    for step, batch in enumerate(test_loader):\n",
    "        if step * batch_size < n:\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ids = batch['id'].tolist()\n",
    "            questions_emb = [questions_emb_list[i] for i in ids]\n",
    "            # 执行两阶段检索，获取额外知识、掩码和检索到的描述\n",
    "            test_extra_knowledges, _, masks,retri_des = retrieval_func_two_stage(questions_emb, doc_embed_tensor, KG_lists, SG_edges,des,device=model.device(),topk_n=20,intervals=0)\n",
    "            # 使用模型进行文本推理\n",
    "            output = model.inference_text(batch,retri_des)\n",
    "            for j in range(len(output['questions'])):\n",
    "                predictions.append(output['pred'][j]) # 添加预测结果\n",
    "                labels.append(output['answers'][j]) # 添加真实答案\n",
    "\n",
    "    # 将预测结果保存到pkl文件\n",
    "    with open('./answer_list/ours_hotpop.pkl', 'wb') as file2:\n",
    "        pickle.dump(predictions,file2)\n",
    "\n",
    "    current_time = datetime.datetime.now()\n",
    "    print('Times:', current_time) # 再次打印当前时间\n",
    "\n",
    "    # 计算精确匹配(Exact Match)率\n",
    "    Truenums1 = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if labels[i].lower() in predictions[i].lower():\n",
    "            Truenums1 += 1\n",
    "    print('EM: ',Truenums1/len(predictions))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args_llama()\n",
    "\n",
    "    main(args)\n",
    "    \n",
    "    torch.cuda.empty_cache()                # 清空CUDA缓存\n",
    "    torch.cuda.reset_max_memory_allocated() # 重置CUDA最大内存分配记录\n",
    "    gc.collect()                            # 执行垃圾回收"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9917278d",
   "metadata": {},
   "source": [
    "# 2. 知识检索组件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb844aa3",
   "metadata": {},
   "source": [
    "### 1. src/retrieval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1197972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "retrieval_func:\n",
    "# for i in range(len(neighbors)):\n",
    "#     for edge in neighbors[i]:\n",
    "#         if int(edge[1]) not in selected_SG[i]:\n",
    "#             selected_SG[i].append(int(edge[1]))\n",
    "\n",
    "retrieval_func_two_stage:\n",
    "#         for i in range(len(neighbors)):\n",
    "#             for edge in neighbors[i]:\n",
    "#                 for j in range(1,len(edge)):\n",
    "#                     if int(edge[j]) not in selected_SG[i]:\n",
    "#                         selected_SG[i].append(int(edge[j]))\n",
    "+++\n",
    "# if (values[0] + pedal[j] in selected_node or values[1] + pedal[j] in selected_node) and str(\n",
    "#     int(values[0])) + ',' + str(int(values[1])) in target_des.keys():\n",
    "\n",
    "def retrieval_func_with_att:\n",
    "# if (values[0] + pedal[j] in selected_node or values[1] + pedal[j] in selected_node) and str(\n",
    "#     int(values[0])) + ',' + str(int(values[1])) in target_des.keys():\n",
    "\n",
    "retrieval_func_via_doc:\n",
    "空？？？\n",
    "\n",
    "'''\n",
    "def retrieval_func(questions_emb,SG_lists,KG_lists,SG_edges,des,device,topk_n=7,topk_e=3,intervals=0.40):\n",
    "    questions_emb = torch.stack(questions_emb).to(device)\n",
    "    SG_lists = torch.stack(SG_lists).to(device)\n",
    "    inner_product = torch.matmul(questions_emb, SG_lists.T)\n",
    "    l2_norm1 = torch.norm(questions_emb, p=2, dim=1, keepdim=True)\n",
    "    l2_norm2 = torch.norm(SG_lists, p=2, dim=1, keepdim=True)\n",
    "    sg_prizes = inner_product / (l2_norm1 * l2_norm2.T)\n",
    "\n",
    "    topk_sg_values, topk_sg_indices = torch.topk(sg_prizes, 7, largest=True)\n",
    "    selected_SG = topk_sg_indices.cpu().tolist()\n",
    "\n",
    "    neighbors = SG_edges.T[topk_sg_indices.cpu().numpy()]\n",
    "    for i in range(len(neighbors)):\n",
    "        for edge in neighbors[i]:\n",
    "            if int(edge[1]) not in selected_SG[i]:\n",
    "                selected_SG[i].append(int(edge[1]))\n",
    "\n",
    "    ret_data = []\n",
    "    ret_mask = []\n",
    "    ret_des = []\n",
    "    for i in range(len(selected_SG)):\n",
    "        KGs_x = [KG_lists[j].x for j in selected_SG[i]]\n",
    "        KGs_edge_attr = [KG_lists[j].edge_attr for j in selected_SG[i]]\n",
    "        KGs_edge_index = [KG_lists[j].edge_index for j in selected_SG[i]]\n",
    "        KGs_numnodes = [KG_lists[j].num_nodes for j in selected_SG[i]]\n",
    "\n",
    "        inner_product = torch.matmul(questions_emb[i], torch.cat(KGs_x, dim=0).T.to(device))\n",
    "        l2_norm1 = torch.norm(questions_emb[i], p=2, dim=0, keepdim=True)\n",
    "        l2_norm2 = torch.norm(torch.cat(KGs_x, dim=0).to(device), p=2, dim=1, keepdim=True).squeeze()\n",
    "        n_prizes = inner_product / (l2_norm1 * l2_norm2)\n",
    "        topk_n = min(len(n_prizes), topk_n)\n",
    "        topk_n_values, topk_n_indices = torch.topk(n_prizes,topk_n , largest=True)\n",
    "        selected_node = []\n",
    "        for j in range(len(topk_n_indices)):\n",
    "            if topk_n_values[j] > intervals:\n",
    "                selected_node.append(topk_n_indices[j])\n",
    "\n",
    "        pedal = [0]\n",
    "        for j in range(len(KGs_numnodes)-1):\n",
    "            pedal.append(pedal[-1]+KGs_numnodes[j])\n",
    "\n",
    "        e = []\n",
    "        e_attr = []\n",
    "        tmp_des = []\n",
    "        for j in range(len(KGs_edge_index)):\n",
    "            target_des = des[int(selected_SG[i][j])]\n",
    "            if not target_des:\n",
    "                target_des = des[int(selected_SG[i][j])-1]\n",
    "            for key,values in enumerate(KGs_edge_index[j].T):\n",
    "                if (values[0] + pedal[j] in selected_node or values[1] + pedal[j] in selected_node) and str(\n",
    "                    int(values[0]))+','+ str(int(values[1])) in target_des.keys():\n",
    "                    e.append(values)\n",
    "                    e_attr.append(KGs_edge_attr[j][key])\n",
    "                    tmp_des.append(target_des[str(int(values[0]))+','+ str(int(values[1]))])\n",
    "        ret_des.append(','.join(tmp_des))\n",
    "\n",
    "    return ret_data, selected_SG,ret_mask,ret_des\n",
    " \n",
    "def retrieval_func_two_stage(questions_emb,doc_embed_tensor,KG_lists,SG_edges,des,device,topk_n=7,topk_e=3,intervals=0.40):\n",
    "    questions_emb = torch.stack(questions_emb).to(device)\n",
    "    SG_lists = doc_embed_tensor.to(device)\n",
    "    inner_product = torch.matmul(questions_emb, SG_lists.T)\n",
    "    l2_norm1 = torch.norm(questions_emb, p=2, dim=1, keepdim=True)\n",
    "    l2_norm2 = torch.norm(SG_lists, p=2, dim=1, keepdim=True)\n",
    "    sg_prizes = inner_product / (l2_norm1 * l2_norm2.T)\n",
    "\n",
    "    topk_sg_values, topk_sg_indices = torch.topk(sg_prizes, 3, largest=True)\n",
    "    selected_SG = topk_sg_indices.cpu().tolist()\n",
    "\n",
    "    '''\n",
    "    neighbors = SG_edges.T[topk_sg_indices.cpu().numpy()]\n",
    "        for i in range(len(neighbors)):\n",
    "            for edge in neighbors[i]:\n",
    "                for j in range(1,len(edge)):\n",
    "                    if int(edge[j]) not in selected_SG[i]:\n",
    "                        selected_SG[i].append(int(edge[j]))\n",
    "    '''\n",
    "    neighbors = SG_edges.T[topk_sg_indices.cpu().numpy()]\n",
    "    for i in range(len(neighbors)):\n",
    "        for edge in neighbors[i]:\n",
    "            for j in range(1,len(edge)):\n",
    "                if int(edge[j]) not in selected_SG[i]:\n",
    "                    selected_SG[i].append(int(edge[j]))\n",
    "\n",
    "    ret_data = []\n",
    "    ret_mask = []\n",
    "    ret_des = []\n",
    "    for i in range(len(selected_SG)):\n",
    "        KGs_x = [KG_lists[j].x for j in selected_SG[i]]\n",
    "        KGs_edge_attr = [KG_lists[j].edge_attr for j in selected_SG[i]]\n",
    "        KGs_edge_index = [KG_lists[j].edge_index for j in selected_SG[i]]\n",
    "        KGs_numnodes = [KG_lists[j].num_nodes for j in selected_SG[i]]\n",
    "\n",
    "        inner_product = torch.matmul(questions_emb[i], torch.cat(KGs_x, dim=0).T.to(device))\n",
    "        l2_norm1 = torch.norm(questions_emb[i], p=2, dim=0, keepdim=True)\n",
    "        l2_norm2 = torch.norm(torch.cat(KGs_x, dim=0).to(device), p=2, dim=1, keepdim=True).squeeze()\n",
    "        n_prizes = inner_product / (l2_norm1 * l2_norm2)\n",
    "        topk_n = min(len(n_prizes), topk_n)\n",
    "        topk_n_values, topk_n_indices = torch.topk(n_prizes, topk_n, largest=True)\n",
    "        selected_node = []\n",
    "        for j in range(len(topk_n_indices)):\n",
    "            if topk_n_values[j] > intervals:\n",
    "                selected_node.append(topk_n_indices[j])\n",
    "\n",
    "        pedal = [0]\n",
    "        for j in range(len(KGs_numnodes) - 1):\n",
    "            pedal.append(pedal[-1] + KGs_numnodes[j])\n",
    "\n",
    "        e = []\n",
    "        e_attr = []\n",
    "        tmp_des = []\n",
    "        for j in range(len(KGs_edge_index)):\n",
    "            target_des = des[int(selected_SG[i][j])]\n",
    "            if not target_des:\n",
    "                target_des = des[int(selected_SG[i][j]) - 1]\n",
    "            for key, values in enumerate(KGs_edge_index[j].T):\n",
    "                '''\n",
    "                IF selected_node AND target_des(selected_SG)\n",
    "                '''\n",
    "                if (values[0] + pedal[j] in selected_node or values[1] + pedal[j] in selected_node) and str(\n",
    "                        int(values[0])) + ',' + str(int(values[1])) in target_des.keys():\n",
    "                    e.append(values)\n",
    "                    e_attr.append(KGs_edge_attr[j][key])\n",
    "                    tmp_des.append(target_des[str(int(values[0])) + ',' + str(int(values[1]))])\n",
    "        ret_des.append(','.join(tmp_des))\n",
    "\n",
    "    return ret_data, selected_SG, ret_mask, ret_des\n",
    "\n",
    "def retrieval_func_with_att(questions_emb,doc_embed_tensor,KG_lists,SG,des,device,topk_n=7,topk_e=3,intervals=0.40):\n",
    "    questions_emb = torch.stack(questions_emb).to(device)\n",
    "    SG_lists = doc_embed_tensor.to(device)\n",
    "    inner_product = torch.matmul(questions_emb, SG_lists.T)\n",
    "    l2_norm1 = torch.norm(questions_emb, p=2, dim=1, keepdim=True)\n",
    "    l2_norm2 = torch.norm(SG_lists, p=2, dim=1, keepdim=True)\n",
    "    sg_prizes = inner_product / (l2_norm1 * l2_norm2.T)\n",
    "\n",
    "    k = 4\n",
    "    topk_sg_values, topk_sg_indices = torch.topk(sg_prizes, k, largest=True)\n",
    "    selected_SG = topk_sg_indices.cpu().tolist()\n",
    "    weight_list = [1]*k\n",
    "\n",
    "    new_node = []\n",
    "    for node in selected_SG[0]:\n",
    "        for neib in list(nx.neighbors(SG, node)):  # find 1_th neighbors\n",
    "            if neib not in selected_SG[0] and neib not in new_node:\n",
    "                new_node.append(neib)\n",
    "                weight_list.append(SG[node][neib]['weight'])\n",
    "    new_new_node = new_node.copy()\n",
    "\n",
    "    selected_SG[0] += new_new_node\n",
    "\n",
    "    ret_data = []\n",
    "    ret_mask = []\n",
    "    ret_des = []\n",
    "    for i in range(len(selected_SG)):\n",
    "        KGs_x = [KG_lists[j].x for j in selected_SG[i]]\n",
    "        KGs_edge_attr = [KG_lists[j].edge_attr for j in selected_SG[i]]\n",
    "        KGs_edge_index = [KG_lists[j].edge_index for j in selected_SG[i]]\n",
    "        KGs_numnodes = [KG_lists[j].num_nodes for j in selected_SG[i]]\n",
    "\n",
    "        attention = []\n",
    "        for j in range(len(weight_list)):\n",
    "            attention += [weight_list[j]] * KGs_numnodes[j]\n",
    "\n",
    "        inner_product = torch.matmul(questions_emb[i], torch.cat(KGs_x, dim=0).T.to(device))\n",
    "        l2_norm1 = torch.norm(questions_emb[i], p=2, dim=0, keepdim=True)\n",
    "        l2_norm2 = torch.norm(torch.cat(KGs_x, dim=0).to(device), p=2, dim=1, keepdim=True).squeeze()\n",
    "        n_prizes = inner_product / (l2_norm1 * l2_norm2)\n",
    "        topk_n = min(len(n_prizes), topk_n)\n",
    "        topk_n_values, topk_n_indices = torch.topk(torch.stack([n_prizes[j] * attention[j] for j in range(len(attention))]), topk_n, largest=True)\n",
    "        selected_node = []\n",
    "        for j in range(len(topk_n_indices)):\n",
    "            if topk_n_values[j] > intervals:\n",
    "                selected_node.append(topk_n_indices[j])\n",
    "\n",
    "        pedal = [0]\n",
    "        for j in range(len(KGs_numnodes) - 1):\n",
    "            pedal.append(pedal[-1] + KGs_numnodes[j])\n",
    "\n",
    "        e = []\n",
    "        e_attr = []\n",
    "        tmp_des = []\n",
    "        for j in range(len(KGs_edge_index)):\n",
    "            target_des = des[int(selected_SG[i][j])]\n",
    "            if not target_des:\n",
    "                target_des = des[int(selected_SG[i][j]) - 1]\n",
    "            for key, values in enumerate(KGs_edge_index[j].T):\n",
    "                if (values[0] + pedal[j] in selected_node or values[1] + pedal[j] in selected_node) and str(\n",
    "                        int(values[0])) + ',' + str(int(values[1])) in target_des.keys():\n",
    "                    e.append(values)\n",
    "                    e_attr.append(KGs_edge_attr[j][key])\n",
    "                    tmp_des.append(target_des[str(int(values[0])) + ',' + str(int(values[1]))])\n",
    "        ret_des.append(','.join(tmp_des))\n",
    "\n",
    "    return ret_data, selected_SG, ret_mask, ret_des\n",
    "\n",
    "# 这似乎未完成 ???\n",
    "def retrieval_func_via_doc(questions_emb,doc_embed_list,doc_text_list,device,topk_n=7,topk_e=3,intervals=0.44,SG_edges=None):\n",
    "    # questions_emb 和 graph_emb之间有gap啊\n",
    "    questions_emb = torch.stack(questions_emb).to(device)\n",
    "    # doc_embed = torch.stack(doc_embed_list).to(device)\n",
    "    doc_embed = doc_embed_list.to(device)\n",
    "    inner_product = torch.matmul(questions_emb, doc_embed.T)\n",
    "    l2_norm1 = torch.norm(questions_emb, p=2, dim=1, keepdim=True)\n",
    "    l2_norm2 = torch.norm(doc_embed, p=2, dim=1, keepdim=True)\n",
    "    sg_prizes = inner_product / (l2_norm1 * l2_norm2.T)\n",
    "\n",
    "    topk_sg_values, topk_sg_indices = torch.topk(sg_prizes, 1, largest=True)\n",
    "\n",
    "    selected_SG = topk_sg_indices.cpu().tolist()\n",
    "\n",
    "    ret_data = []\n",
    "    ret_mask = []\n",
    "    ret_des = []\n",
    "    for i in range(len(selected_SG)):\n",
    "        e = []\n",
    "        e_attr = []\n",
    "        tmp_des = []\n",
    "        for j in range(len(selected_SG[i])):\n",
    "            target_des = doc_text_list[int(selected_SG[i][j])]\n",
    "            tmp_des.append(target_des)\n",
    "        ret_des.append(','.join(tmp_des))\n",
    "\n",
    "    return ret_data, selected_SG,ret_mask,ret_des"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
