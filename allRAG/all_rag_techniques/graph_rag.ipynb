{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GraphRAG：图增强检索增强生成\n",
        "\n",
        "## 概述\n",
        "\n",
        "GraphRAG 是一个先进的问答系统，它结合了基于图的知识表示和检索增强生成技术。它对输入文档进行处理，以创建丰富的知识图谱，然后利用该知识图谱增强用户查询的检索和答案生成过程。该系统借助自然语言处理、机器学习和图论，提供更准确且与上下文相关的响应。\n",
        "\n",
        "## 动机\n",
        "\n",
        "传统的检索增强生成系统在处理长文档时，往往难以保持上下文连贯性，也难以在相关信息之间建立联系。GraphRAG 通过以下方式解决了这些局限性：\n",
        "\n",
        "1. 将知识表示为相互关联的图，能够更好地保留概念之间的关系。\n",
        "2. 在查询过程中实现更智能的信息遍历。\n",
        "3. 直观展示回答过程中信息的关联方式和获取路径。\n",
        "\n",
        "## 核心组件\n",
        "\n",
        "1. **DocumentProcessor（文档处理器）**：处理输入文档的初始阶段，创建文本块和嵌入向量。\n",
        "\n",
        "2. **KnowledgeGraph（知识图谱）**：构建处理后文档的图表示，其中节点代表文本块，边代表它们之间的关系。\n",
        "\n",
        "3. **QueryEngine（查询引擎）**：利用知识图谱和向量存储来管理用户查询的回答过程。\n",
        "\n",
        "4. **Visualizer（可视化工具）**：创建图的可视化表示以及回答查询时的遍历路径。\n",
        "\n",
        "## 方法细节\n",
        "\n",
        "1. **文档处理**：\n",
        "   - 将输入文档分割为可管理的文本块。\n",
        "   - 使用语言模型对每个文本块进行嵌入处理。\n",
        "   - 从这些嵌入向量创建向量存储，以实现高效的相似性搜索。\n",
        "\n",
        "2. **知识图谱构建**：\n",
        "   - 为每个文本块创建图节点。\n",
        "   - 结合自然语言处理技术和语言模型从每个文本块中提取概念。\n",
        "   - 对提取的概念进行词形还原，以提高匹配度。\n",
        "   - 基于语义相似性和共享概念在节点之间添加边。\n",
        "   - 计算边的权重以表示关系的强度。\n",
        "\n",
        "3. **查询处理**：\n",
        "   - 对用户查询进行嵌入处理，并用于从向量存储中检索相关文档。\n",
        "   - 用与最相关文档对应的节点初始化优先级队列。\n",
        "   - 系统采用类 Dijkstra 算法遍历知识图谱：\n",
        "     * 按照节点与查询的关联强度（优先级）依次探索节点。\n",
        "     * 对于每个探索的节点：\n",
        "       - 将其内容添加到上下文中。\n",
        "       - 检查当前上下文是否能提供完整的答案。\n",
        "       - 如果答案不完整：\n",
        "         * 处理该节点的概念并添加到已访问概念集。\n",
        "         * 探索相邻节点，并根据边的权重更新它们的优先级。\n",
        "         * 如果发现更强的关联，将节点添加到优先级队列。\n",
        "   - 此过程持续到找到完整答案或优先级队列耗尽为止。\n",
        "   - 如果遍历图后仍未找到完整答案，系统会利用累积的上下文和大型语言模型生成最终答案。\n",
        "\n",
        "4. **可视化**：\n",
        "   - 知识图谱的可视化中，节点代表文本块，边代表关系。\n",
        "   - 边的颜色表示关系的强度（权重）。\n",
        "   - 用弯曲的虚线箭头突出显示回答查询时的遍历路径。\n",
        "   - 遍历的起始节点和结束节点采用不同的颜色，便于识别。\n",
        "\n",
        "## 该方法的优势\n",
        "\n",
        "1. **增强的上下文感知**：通过将知识表示为图，系统能够更好地保持上下文，并在输入文档的不同部分之间建立联系。\n",
        "\n",
        "2. **更优的检索效果**：图结构使信息检索更智能，超越了简单的关键词匹配。\n",
        "\n",
        "3. **可解释的结果**：图和遍历路径的可视化展示了系统得出答案的过程，提高了透明度和可信度。\n",
        "\n",
        "4. **灵活的知识表示**：图结构可以轻松整合新出现的信息和关系。\n",
        "\n",
        "5. **高效的信息遍历**：图中的加权边使系统在回答查询时能够优先选择最相关的信息路径。\n",
        "\n",
        "## 结论\n",
        "\n",
        "GraphRAG 在检索增强生成系统方面取得了显著进步。通过整合基于图的知识表示和智能遍历机制，它提供了更强的上下文感知、更准确的检索和更高的可解释性。该系统可视化决策过程的能力为其运行机制提供了宝贵的见解，使其成为终端用户和开发人员的强大工具。随着自然语言处理和基于图的人工智能不断发展，像 GraphRAG 这样的系统为更复杂、更强大的问答技术铺平了道路。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "<img src=\"../images/graph_rag.svg\" alt=\"graph RAG\" style=\"width:100%; height:auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Package Installation and Imports\n",
        "\n",
        "The cell below installs all necessary packages required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# conda create -p /data0/bma/env/RAG_study python=3.10\n",
        "# conda activate /data0/bma/env/RAG_study\n",
        "# conda deactivate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "# pip install faiss-cpu futures langchain langchain-openai matplotlib networkx nltk numpy python-dotenv scikit-learn spacy tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the repository to access helper functions and evaluation modules\n",
        "# # !git clone https://github.com/NirDiamant/RAG_TECHNIQUES.git\n",
        "# import sys\n",
        "# sys.path.append('RAG_TECHNIQUES')\n",
        "# If you need to run with the latest data\n",
        "# !cp -r RAG_TECHNIQUES/data ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'helper_functions'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m English\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Original path append replaced for Colab compatibility\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhelper_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevalute_rag\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Load environment variables from a .env file\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'helper_functions'"
          ]
        }
      ],
      "source": [
        "import networkx as nx\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from typing import List, Tuple, Dict\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import spacy\n",
        "import heapq\n",
        "\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "from spacy.cli import download\n",
        "from spacy.lang.en import English\n",
        "\n",
        "\n",
        "# Original path append replaced for Colab compatibility\n",
        "from helper_functions import *\n",
        "from evaluation.evalute_rag import *\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Set the OpenAI API key environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the document processor class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the DocumentProcessor class\n",
        "class DocumentProcessor:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the DocumentProcessor with a text splitter and OpenAI embeddings.\n",
        "        \n",
        "        Attributes:\n",
        "        - text_splitter: An instance of RecursiveCharacterTextSplitter with specified chunk size and overlap.\n",
        "        - embeddings: An instance of OpenAIEmbeddings used for embedding documents.\n",
        "        \"\"\"\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "\n",
        "    def process_documents(self, documents):\n",
        "        \"\"\"\n",
        "        Processes a list of documents by splitting them into smaller chunks and creating a vector store.\n",
        "        \n",
        "        Args:\n",
        "        - documents (list of str): A list of documents to be processed.\n",
        "        \n",
        "        Returns:\n",
        "        - tuple: A tuple containing:\n",
        "          - splits (list of str): The list of split document chunks.\n",
        "          - vector_store (FAISS): A FAISS vector store created from the split document chunks and their embeddings.\n",
        "        \"\"\"\n",
        "        splits = self.text_splitter.split_documents(documents)\n",
        "        vector_store = FAISS.from_documents(splits, self.embeddings)\n",
        "        return splits, vector_store\n",
        "\n",
        "    def create_embeddings_batch(self, texts, batch_size=32):\n",
        "        \"\"\"\n",
        "        Creates embeddings for a list of texts in batches.\n",
        "        \n",
        "        Args:\n",
        "        - texts (list of str): A list of texts to be embedded.\n",
        "        - batch_size (int, optional): The number of texts to process in each batch. Default is 32.\n",
        "        \n",
        "        Returns:\n",
        "        - numpy.ndarray: An array of embeddings for the input texts.\n",
        "        \"\"\"\n",
        "        embeddings = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = texts[i:i+batch_size]\n",
        "            batch_embeddings = self.embeddings.embed_documents(batch)\n",
        "            embeddings.extend(batch_embeddings)\n",
        "        return np.array(embeddings)\n",
        "\n",
        "    def compute_similarity_matrix(self, embeddings):\n",
        "        \"\"\"\n",
        "        Computes a cosine similarity matrix for a given set of embeddings.\n",
        "        \n",
        "        Args:\n",
        "        - embeddings (numpy.ndarray): An array of embeddings.\n",
        "        \n",
        "        Returns:\n",
        "        - numpy.ndarray: A cosine similarity matrix for the input embeddings.\n",
        "        \"\"\"\n",
        "        return cosine_similarity(embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the knowledge graph class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the Concepts class\n",
        "class Concepts(BaseModel):\n",
        "    concepts_list: List[str] = Field(description=\"List of concepts\")\n",
        "\n",
        "# Define the KnowledgeGraph class\n",
        "class KnowledgeGraph:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the KnowledgeGraph with a graph, lemmatizer, and NLP model.\n",
        "        \n",
        "        Attributes:\n",
        "        - graph: An instance of a networkx Graph.\n",
        "        - lemmatizer: An instance of WordNetLemmatizer.\n",
        "        - concept_cache: A dictionary to cache extracted concepts.\n",
        "        - nlp: An instance of a spaCy NLP model.\n",
        "        - edges_threshold: A float value that sets the threshold for adding edges based on similarity.\n",
        "        \"\"\"\n",
        "        self.graph = nx.Graph()\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.concept_cache = {}\n",
        "        self.nlp = self._load_spacy_model()\n",
        "        self.edges_threshold = 0.8\n",
        "\n",
        "    def build_graph(self, splits, llm, embedding_model):\n",
        "        \"\"\"\n",
        "        Builds the knowledge graph by adding nodes, creating embeddings, extracting concepts, and adding edges.\n",
        "        \n",
        "        Args:\n",
        "        - splits (list): A list of document splits.\n",
        "        - llm: An instance of a large language model.\n",
        "        - embedding_model: An instance of an embedding model.\n",
        "        \n",
        "        Returns:\n",
        "        - None\n",
        "        \"\"\"\n",
        "        self._add_nodes(splits)\n",
        "        embeddings = self._create_embeddings(splits, embedding_model)\n",
        "        self._extract_concepts(splits, llm)\n",
        "        self._add_edges(embeddings)\n",
        "\n",
        "    def _add_nodes(self, splits):\n",
        "        \"\"\"\n",
        "        Adds nodes to the graph from the document splits.\n",
        "        \n",
        "        Args:\n",
        "        - splits (list): A list of document splits.\n",
        "        \n",
        "        Returns:\n",
        "        - None\n",
        "        \"\"\"\n",
        "        for i, split in enumerate(splits):\n",
        "            self.graph.add_node(i, content=split.page_content)\n",
        "\n",
        "    def _create_embeddings(self, splits, embedding_model):\n",
        "        \"\"\"\n",
        "        Creates embeddings for the document splits using the embedding model.\n",
        "        \n",
        "        Args:\n",
        "        - splits (list): A list of document splits.\n",
        "        - embedding_model: An instance of an embedding model.\n",
        "        \n",
        "        Returns:\n",
        "        - numpy.ndarray: An array of embeddings for the document splits.\n",
        "        \"\"\"\n",
        "        texts = [split.page_content for split in splits]\n",
        "        return embedding_model.embed_documents(texts)\n",
        "\n",
        "    def _compute_similarities(self, embeddings):\n",
        "        \"\"\"\n",
        "        Computes the cosine similarity matrix for the embeddings.\n",
        "        \n",
        "        Args:\n",
        "        - embeddings (numpy.ndarray): An array of embeddings.\n",
        "        \n",
        "        Returns:\n",
        "        - numpy.ndarray: A cosine similarity matrix for the embeddings.\n",
        "        \"\"\"\n",
        "        return cosine_similarity(embeddings)\n",
        "\n",
        "    def _load_spacy_model(self):\n",
        "        \"\"\"\n",
        "        Loads the spaCy NLP model, downloading it if necessary.\n",
        "        \n",
        "        Args:\n",
        "        - None\n",
        "        \n",
        "        Returns:\n",
        "        - spacy.Language: An instance of a spaCy NLP model.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return spacy.load(\"en_core_web_sm\")\n",
        "        except OSError:\n",
        "            print(\"Downloading spaCy model...\")\n",
        "            download(\"en_core_web_sm\")\n",
        "            return spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    def _extract_concepts_and_entities(self, content, llm):\n",
        "        \"\"\"\n",
        "        Extracts concepts and named entities from the content using spaCy and a large language model.\n",
        "        \n",
        "        Args:\n",
        "        - content (str): The content from which to extract concepts and entities.\n",
        "        - llm: An instance of a large language model.\n",
        "        \n",
        "        Returns:\n",
        "        - list: A list of extracted concepts and entities.\n",
        "        \"\"\"\n",
        "        if content in self.concept_cache:\n",
        "            return self.concept_cache[content]\n",
        "        \n",
        "        # Extract named entities using spaCy\n",
        "        doc = self.nlp(content)\n",
        "        named_entities = [ent.text for ent in doc.ents if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\", \"WORK_OF_ART\"]]\n",
        "        \n",
        "        # Extract general concepts using LLM\n",
        "        concept_extraction_prompt = PromptTemplate(\n",
        "            input_variables=[\"text\"],\n",
        "            template=\"Extract key concepts (excluding named entities) from the following text:\\n\\n{text}\\n\\nKey concepts:\"\n",
        "        )\n",
        "        concept_chain = concept_extraction_prompt | llm.with_structured_output(Concepts)\n",
        "        general_concepts = concept_chain.invoke({\"text\": content}).concepts_list\n",
        "        \n",
        "        # Combine named entities and general concepts\n",
        "        all_concepts = list(set(named_entities + general_concepts))\n",
        "        \n",
        "        self.concept_cache[content] = all_concepts\n",
        "        return all_concepts\n",
        "\n",
        "    def _extract_concepts(self, splits, llm):\n",
        "        \"\"\"\n",
        "        Extracts concepts for all document splits using multi-threading.\n",
        "        \n",
        "        Args:\n",
        "        - splits (list): A list of document splits.\n",
        "        - llm: An instance of a large language model.\n",
        "        \n",
        "        Returns:\n",
        "        - None\n",
        "        \"\"\"\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            future_to_node = {executor.submit(self._extract_concepts_and_entities, split.page_content, llm): i \n",
        "                              for i, split in enumerate(splits)}\n",
        "            \n",
        "            for future in tqdm(as_completed(future_to_node), total=len(splits), desc=\"Extracting concepts and entities\"):\n",
        "                node = future_to_node[future]\n",
        "                concepts = future.result()\n",
        "                self.graph.nodes[node]['concepts'] = concepts\n",
        "\n",
        "    def _add_edges(self, embeddings):\n",
        "        \"\"\"\n",
        "        Adds edges to the graph based on the similarity of embeddings and shared concepts.\n",
        "        \n",
        "        Args:\n",
        "        - embeddings (numpy.ndarray): An array of embeddings for the document splits.\n",
        "        \n",
        "        Returns:\n",
        "        - None\n",
        "        \"\"\"\n",
        "        similarity_matrix = self._compute_similarities(embeddings)\n",
        "        num_nodes = len(self.graph.nodes)\n",
        "        \n",
        "        for node1 in tqdm(range(num_nodes), desc=\"Adding edges\"):\n",
        "            for node2 in range(node1 + 1, num_nodes):\n",
        "                similarity_score = similarity_matrix[node1][node2]\n",
        "                if similarity_score > self.edges_threshold:\n",
        "                    shared_concepts = set(self.graph.nodes[node1]['concepts']) & set(self.graph.nodes[node2]['concepts'])\n",
        "                    edge_weight = self._calculate_edge_weight(node1, node2, similarity_score, shared_concepts)\n",
        "                    self.graph.add_edge(node1, node2, weight=edge_weight, \n",
        "                                        similarity=similarity_score,\n",
        "                                        shared_concepts=list(shared_concepts))\n",
        "\n",
        "    def _calculate_edge_weight(self, node1, node2, similarity_score, shared_concepts, alpha=0.7, beta=0.3):\n",
        "        \"\"\"\n",
        "        Calculates the weight of an edge based on similarity score and shared concepts.\n",
        "        \n",
        "        Args:\n",
        "        - node1 (int): The first node.\n",
        "        - node2 (int): The second node.\n",
        "        - similarity_score (float): The similarity score between the nodes.\n",
        "        - shared_concepts (set): The set of shared concepts between the nodes.\n",
        "        - alpha (float, optional): The weight of the similarity score. Default is 0.7.\n",
        "        - beta (float, optional): The weight of the shared concepts. Default is 0.3.\n",
        "        \n",
        "        Returns:\n",
        "        - float: The calculated weight of the edge.\n",
        "        \"\"\"\n",
        "        max_possible_shared = min(len(self.graph.nodes[node1]['concepts']), len(self.graph.nodes[node2]['concepts']))\n",
        "        normalized_shared_concepts = len(shared_concepts) / max_possible_shared if max_possible_shared > 0 else 0\n",
        "        return alpha * similarity_score + beta * normalized_shared_concepts\n",
        "\n",
        "    def _lemmatize_concept(self, concept):\n",
        "        \"\"\"\n",
        "        Lemmatizes a given concept.\n",
        "        \n",
        "        Args:\n",
        "        - concept (str): The concept to be lemmatized.\n",
        "        \n",
        "        Returns:\n",
        "        - str: The lemmatized concept.\n",
        "        \"\"\"\n",
        "        return ' '.join([self.lemmatizer.lemmatize(word) for word in concept.lower().split()])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the Query Engine class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download required data files\n",
        "import os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Download the PDF document used in this notebook\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Define the AnswerCheck class\n",
        "class AnswerCheck(BaseModel):\n",
        "    is_complete: bool = Field(description=\"Whether the current context provides a complete answer to the query\")\n",
        "    answer: str = Field(description=\"The current answer based on the context, if any\")\n",
        "\n",
        "# Define the QueryEngine class\n",
        "class QueryEngine:\n",
        "    def __init__(self, vector_store, knowledge_graph, llm):\n",
        "        self.vector_store = vector_store\n",
        "        self.knowledge_graph = knowledge_graph\n",
        "        self.llm = llm\n",
        "        self.max_context_length = 4000\n",
        "        self.answer_check_chain = self._create_answer_check_chain()\n",
        "\n",
        "    def _create_answer_check_chain(self):\n",
        "        \"\"\"\n",
        "        Creates a chain to check if the context provides a complete answer to the query.\n",
        "        \n",
        "        Args:\n",
        "        - None\n",
        "        \n",
        "        Returns:\n",
        "        - Chain: A chain to check if the context provides a complete answer.\n",
        "        \"\"\"\n",
        "        answer_check_prompt = PromptTemplate(\n",
        "            input_variables=[\"query\", \"context\"],\n",
        "            template=\"Given the query: '{query}'\\n\\nAnd the current context:\\n{context}\\n\\nDoes this context provide a complete answer to the query? If yes, provide the answer. If no, state that the answer is incomplete.\\n\\nIs complete answer (Yes/No):\\nAnswer (if complete):\"\n",
        "        )\n",
        "        return answer_check_prompt | self.llm.with_structured_output(AnswerCheck)\n",
        "\n",
        "    def _check_answer(self, query: str, context: str) -> Tuple[bool, str]:\n",
        "        \"\"\"\n",
        "        Checks if the current context provides a complete answer to the query.\n",
        "        \n",
        "        Args:\n",
        "        - query (str): The query to be answered.\n",
        "        - context (str): The current context.\n",
        "        \n",
        "        Returns:\n",
        "        - tuple: A tuple containing:\n",
        "          - is_complete (bool): Whether the context provides a complete answer.\n",
        "          - answer (str): The answer based on the context, if complete.\n",
        "        \"\"\"\n",
        "        response = self.answer_check_chain.invoke({\"query\": query, \"context\": context})\n",
        "        return response.is_complete, response.answer\n",
        "\n",
        "  \n",
        "\n",
        "    def _expand_context(self, query: str, relevant_docs) -> Tuple[str, List[int], Dict[int, str], str]:\n",
        "        \"\"\"\n",
        "        Expands the context by traversing the knowledge graph using a Dijkstra-like approach.\n",
        "        \n",
        "        This method implements a modified version of Dijkstra's algorithm to explore the knowledge graph,\n",
        "        prioritizing the most relevant and strongly connected information. The algorithm works as follows:\n",
        "\n",
        "        1. Initialize:\n",
        "           - Start with nodes corresponding to the most relevant documents.\n",
        "           - Use a priority queue to manage the traversal order, where priority is based on connection strength.\n",
        "           - Maintain a dictionary of best known \"distances\" (inverse of connection strengths) to each node.\n",
        "\n",
        "        2. Traverse:\n",
        "           - Always explore the node with the highest priority (strongest connection) next.\n",
        "           - For each node, check if we've found a complete answer.\n",
        "           - Explore the node's neighbors, updating their priorities if a stronger connection is found.\n",
        "\n",
        "        3. Concept Handling:\n",
        "           - Track visited concepts to guide the exploration towards new, relevant information.\n",
        "           - Expand to neighbors only if they introduce new concepts.\n",
        "\n",
        "        4. Termination:\n",
        "           - Stop if a complete answer is found.\n",
        "           - Continue until the priority queue is empty (all reachable nodes explored).\n",
        "\n",
        "        This approach ensures that:\n",
        "        - We prioritize the most relevant and strongly connected information.\n",
        "        - We explore new concepts systematically.\n",
        "        - We find the most relevant answer by following the strongest connections in the knowledge graph.\n",
        "\n",
        "        Args:\n",
        "        - query (str): The query to be answered.\n",
        "        - relevant_docs (List[Document]): A list of relevant documents to start the traversal.\n",
        "\n",
        "        Returns:\n",
        "        - tuple: A tuple containing:\n",
        "          - expanded_context (str): The accumulated context from traversed nodes.\n",
        "          - traversal_path (List[int]): The sequence of node indices visited.\n",
        "          - filtered_content (Dict[int, str]): A mapping of node indices to their content.\n",
        "          - final_answer (str): The final answer found, if any.\n",
        "        \"\"\"\n",
        "        # Initialize variables\n",
        "        expanded_context = \"\"\n",
        "        traversal_path = []\n",
        "        visited_concepts = set()\n",
        "        filtered_content = {}\n",
        "        final_answer = \"\"\n",
        "        \n",
        "        priority_queue = []\n",
        "        distances = {}  # Stores the best known \"distance\" (inverse of connection strength) to each node\n",
        "        \n",
        "        print(\"\\nTraversing the knowledge graph:\")\n",
        "        \n",
        "        # Initialize priority queue with closest nodes from relevant docs\n",
        "        for doc in relevant_docs:\n",
        "            # Find the most similar node in the knowledge graph for each relevant document\n",
        "            closest_nodes = self.vector_store.similarity_search_with_score(doc.page_content, k=1)\n",
        "            closest_node_content, similarity_score = closest_nodes[0]\n",
        "            \n",
        "            # Get the corresponding node in our knowledge graph\n",
        "            closest_node = next(n for n in self.knowledge_graph.graph.nodes if self.knowledge_graph.graph.nodes[n]['content'] == closest_node_content.page_content)\n",
        "            \n",
        "            # Initialize priority (inverse of similarity score for min-heap behavior)\n",
        "            priority = 1 / similarity_score\n",
        "            heapq.heappush(priority_queue, (priority, closest_node))\n",
        "            distances[closest_node] = priority\n",
        "        \n",
        "        step = 0\n",
        "        while priority_queue:\n",
        "            # Get the node with the highest priority (lowest distance value)\n",
        "            current_priority, current_node = heapq.heappop(priority_queue)\n",
        "            \n",
        "            # Skip if we've already found a better path to this node\n",
        "            if current_priority > distances.get(current_node, float('inf')):\n",
        "                continue\n",
        "            \n",
        "            if current_node not in traversal_path:\n",
        "                step += 1\n",
        "                traversal_path.append(current_node)\n",
        "                node_content = self.knowledge_graph.graph.nodes[current_node]['content']\n",
        "                node_concepts = self.knowledge_graph.graph.nodes[current_node]['concepts']\n",
        "                \n",
        "                # Add node content to our accumulated context\n",
        "                filtered_content[current_node] = node_content\n",
        "                expanded_context += \"\\n\" + node_content if expanded_context else node_content\n",
        "                \n",
        "                # Log the current step for debugging and visualization\n",
        "                print(f\"\\nStep {step} - Node {current_node}:\")\n",
        "                print(f\"Content: {node_content[:100]}...\") \n",
        "                print(f\"Concepts: {', '.join(node_concepts)}\")\n",
        "                print(\"-\" * 50)\n",
        "                \n",
        "                # Check if we have a complete answer with the current context\n",
        "                is_complete, answer = self._check_answer(query, expanded_context)\n",
        "                if is_complete:\n",
        "                    final_answer = answer\n",
        "                    break\n",
        "                \n",
        "                # Process the concepts of the current node\n",
        "                node_concepts_set = set(self.knowledge_graph._lemmatize_concept(c) for c in node_concepts)\n",
        "                if not node_concepts_set.issubset(visited_concepts):\n",
        "                    visited_concepts.update(node_concepts_set)\n",
        "                    \n",
        "                    # Explore neighbors\n",
        "                    for neighbor in self.knowledge_graph.graph.neighbors(current_node):\n",
        "                        edge_data = self.knowledge_graph.graph[current_node][neighbor]\n",
        "                        edge_weight = edge_data['weight']\n",
        "                        \n",
        "                        # Calculate new distance (priority) to the neighbor\n",
        "                        # Note: We use 1 / edge_weight because higher weights mean stronger connections\n",
        "                        distance = current_priority + (1 / edge_weight)\n",
        "                        \n",
        "                        # If we've found a stronger connection to the neighbor, update its distance\n",
        "                        if distance < distances.get(neighbor, float('inf')):\n",
        "                            distances[neighbor] = distance\n",
        "                            heapq.heappush(priority_queue, (distance, neighbor))\n",
        "                            \n",
        "                            # Process the neighbor node if it's not already in our traversal path\n",
        "                            if neighbor not in traversal_path:\n",
        "                                step += 1\n",
        "                                traversal_path.append(neighbor)\n",
        "                                neighbor_content = self.knowledge_graph.graph.nodes[neighbor]['content']\n",
        "                                neighbor_concepts = self.knowledge_graph.graph.nodes[neighbor]['concepts']\n",
        "                                \n",
        "                                filtered_content[neighbor] = neighbor_content\n",
        "                                expanded_context += \"\\n\" + neighbor_content if expanded_context else neighbor_content\n",
        "                                \n",
        "                                # Log the neighbor node information\n",
        "                                print(f\"\\nStep {step} - Node {neighbor} (neighbor of {current_node}):\")\n",
        "                                print(f\"Content: {neighbor_content[:100]}...\")\n",
        "                                print(f\"Concepts: {', '.join(neighbor_concepts)}\")\n",
        "                                print(\"-\" * 50)\n",
        "                                \n",
        "                                # Check if we have a complete answer after adding the neighbor's content\n",
        "                                is_complete, answer = self._check_answer(query, expanded_context)\n",
        "                                if is_complete:\n",
        "                                    final_answer = answer\n",
        "                                    break\n",
        "                                \n",
        "                                # Process the neighbor's concepts\n",
        "                                neighbor_concepts_set = set(self.knowledge_graph._lemmatize_concept(c) for c in neighbor_concepts)\n",
        "                                if not neighbor_concepts_set.issubset(visited_concepts):\n",
        "                                    visited_concepts.update(neighbor_concepts_set)\n",
        "                \n",
        "                # If we found a final answer, break out of the main loop\n",
        "                if final_answer:\n",
        "                    break\n",
        "\n",
        "        # If we haven't found a complete answer, generate one using the LLM\n",
        "        if not final_answer:\n",
        "            print(\"\\nGenerating final answer...\")\n",
        "            response_prompt = PromptTemplate(\n",
        "                input_variables=[\"query\", \"context\"],\n",
        "                template=\"Based on the following context, please answer the query.\\n\\nContext: {context}\\n\\nQuery: {query}\\n\\nAnswer:\"\n",
        "            )\n",
        "            response_chain = response_prompt | self.llm\n",
        "            input_data = {\"query\": query, \"context\": expanded_context}\n",
        "            final_answer = response_chain.invoke(input_data)\n",
        "\n",
        "        return expanded_context, traversal_path, filtered_content, final_answer\n",
        "\n",
        "    def query(self, query: str) -> Tuple[str, List[int], Dict[int, str]]:\n",
        "        \"\"\"\n",
        "        Processes a query by retrieving relevant documents, expanding the context, and generating the final answer.\n",
        "        \n",
        "        Args:\n",
        "        - query (str): The query to be answered.\n",
        "        \n",
        "        Returns:\n",
        "        - tuple: A tuple containing:\n",
        "          - final_answer (str): The final answer to the query.\n",
        "          - traversal_path (list): The traversal path of nodes in the knowledge graph.\n",
        "          - filtered_content (dict): The filtered content of nodes.\n",
        "        \"\"\"\n",
        "        with get_openai_callback() as cb:\n",
        "            print(f\"\\nProcessing query: {query}\")\n",
        "            relevant_docs = self._retrieve_relevant_documents(query)\n",
        "            expanded_context, traversal_path, filtered_content, final_answer = self._expand_context(query, relevant_docs)\n",
        "            \n",
        "            if not final_answer:\n",
        "                print(\"\\nGenerating final answer...\")\n",
        "                response_prompt = PromptTemplate(\n",
        "                    input_variables=[\"query\", \"context\"],\n",
        "                    template=\"Based on the following context, please answer the query.\\n\\nContext: {context}\\n\\nQuery: {query}\\n\\nAnswer:\"\n",
        "                )\n",
        "                \n",
        "                response_chain = response_prompt | self.llm\n",
        "                input_data = {\"query\": query, \"context\": expanded_context}\n",
        "                response = response_chain.invoke(input_data)\n",
        "                final_answer = response\n",
        "            else:\n",
        "                print(\"\\nComplete answer found during traversal.\")\n",
        "            \n",
        "            print(f\"\\nFinal Answer: {final_answer}\")\n",
        "            print(f\"\\nTotal Tokens: {cb.total_tokens}\")\n",
        "            print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
        "            print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
        "            print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
        "        \n",
        "        return final_answer, traversal_path, filtered_content\n",
        "\n",
        "    def _retrieve_relevant_documents(self, query: str):\n",
        "        \"\"\"\n",
        "        Retrieves relevant documents based on the query using the vector store.\n",
        "        \n",
        "        Args:\n",
        "        - query (str): The query to be answered.\n",
        "        \n",
        "        Returns:\n",
        "        - list: A list of relevant documents.\n",
        "        \"\"\"\n",
        "        print(\"\\nRetrieving relevant documents...\")\n",
        "        retriever = self.vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
        "        compressor = LLMChainExtractor.from_llm(self.llm)\n",
        "        compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n",
        "        return compression_retriever.invoke(query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the Visualizer class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Define the Visualizer class\n",
        "class Visualizer:\n",
        "    @staticmethod\n",
        "    def visualize_traversal(graph, traversal_path):\n",
        "        \"\"\"\n",
        "        Visualizes the traversal path on the knowledge graph with nodes, edges, and traversal path highlighted.\n",
        "\n",
        "        Args:\n",
        "        - graph (networkx.Graph): The knowledge graph containing nodes and edges.\n",
        "        - traversal_path (list of int): The list of node indices representing the traversal path.\n",
        "\n",
        "        Returns:\n",
        "        - None\n",
        "        \"\"\"\n",
        "        traversal_graph = nx.DiGraph()\n",
        "        \n",
        "        # Add nodes and edges from the original graph\n",
        "        for node in graph.nodes():\n",
        "            traversal_graph.add_node(node)\n",
        "        for u, v, data in graph.edges(data=True):\n",
        "            traversal_graph.add_edge(u, v, **data)\n",
        "        \n",
        "        fig, ax = plt.subplots(figsize=(16, 12))\n",
        "        \n",
        "        # Generate positions for all nodes\n",
        "        pos = nx.spring_layout(traversal_graph, k=1, iterations=50)\n",
        "        \n",
        "        # Draw regular edges with color based on weight\n",
        "        edges = traversal_graph.edges()\n",
        "        edge_weights = [traversal_graph[u][v].get('weight', 0.5) for u, v in edges]\n",
        "        nx.draw_networkx_edges(traversal_graph, pos, \n",
        "                               edgelist=edges,\n",
        "                               edge_color=edge_weights,\n",
        "                               edge_cmap=plt.cm.Blues,\n",
        "                               width=2,\n",
        "                               ax=ax)\n",
        "        \n",
        "        # Draw nodes\n",
        "        nx.draw_networkx_nodes(traversal_graph, pos, \n",
        "                               node_color='lightblue',\n",
        "                               node_size=3000,\n",
        "                               ax=ax)\n",
        "        \n",
        "        # Draw traversal path with curved arrows\n",
        "        edge_offset = 0.1\n",
        "        for i in range(len(traversal_path) - 1):\n",
        "            start = traversal_path[i]\n",
        "            end = traversal_path[i + 1]\n",
        "            start_pos = pos[start]\n",
        "            end_pos = pos[end]\n",
        "            \n",
        "            # Calculate control point for curve\n",
        "            mid_point = ((start_pos[0] + end_pos[0]) / 2, (start_pos[1] + end_pos[1]) / 2)\n",
        "            control_point = (mid_point[0] + edge_offset, mid_point[1] + edge_offset)\n",
        "            \n",
        "            # Draw curved arrow\n",
        "            arrow = patches.FancyArrowPatch(start_pos, end_pos,\n",
        "                                            connectionstyle=f\"arc3,rad={0.3}\",\n",
        "                                            color='red',\n",
        "                                            arrowstyle=\"->\",\n",
        "                                            mutation_scale=20,\n",
        "                                            linestyle='--',\n",
        "                                            linewidth=2,\n",
        "                                            zorder=4)\n",
        "            ax.add_patch(arrow)\n",
        "        \n",
        "        # Prepare labels for the nodes\n",
        "        labels = {}\n",
        "        for i, node in enumerate(traversal_path):\n",
        "            concepts = graph.nodes[node].get('concepts', [])\n",
        "            label = f\"{i + 1}. {concepts[0] if concepts else ''}\"\n",
        "            labels[node] = label\n",
        "        \n",
        "        for node in traversal_graph.nodes():\n",
        "            if node not in labels:\n",
        "                concepts = graph.nodes[node].get('concepts', [])\n",
        "                labels[node] = concepts[0] if concepts else ''\n",
        "        \n",
        "        # Draw labels\n",
        "        nx.draw_networkx_labels(traversal_graph, pos, labels, font_size=8, font_weight=\"bold\", ax=ax)\n",
        "        \n",
        "        # Highlight start and end nodes\n",
        "        start_node = traversal_path[0]\n",
        "        end_node = traversal_path[-1]\n",
        "        \n",
        "        nx.draw_networkx_nodes(traversal_graph, pos, \n",
        "                               nodelist=[start_node], \n",
        "                               node_color='lightgreen', \n",
        "                               node_size=3000,\n",
        "                               ax=ax)\n",
        "        \n",
        "        nx.draw_networkx_nodes(traversal_graph, pos, \n",
        "                               nodelist=[end_node], \n",
        "                               node_color='lightcoral', \n",
        "                               node_size=3000,\n",
        "                               ax=ax)\n",
        "        \n",
        "        ax.set_title(\"Graph Traversal Flow\")\n",
        "        ax.axis('off')\n",
        "        \n",
        "        # Add colorbar for edge weights\n",
        "        sm = plt.cm.ScalarMappable(cmap=plt.cm.Blues, norm=plt.Normalize(vmin=min(edge_weights), vmax=max(edge_weights)))\n",
        "        sm.set_array([])\n",
        "        cbar = fig.colorbar(sm, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
        "        cbar.set_label('Edge Weight', rotation=270, labelpad=15)\n",
        "        \n",
        "        # Add legend\n",
        "        regular_line = plt.Line2D([0], [0], color='blue', linewidth=2, label='Regular Edge')\n",
        "        traversal_line = plt.Line2D([0], [0], color='red', linewidth=2, linestyle='--', label='Traversal Path')\n",
        "        start_point = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightgreen', markersize=15, label='Start Node')\n",
        "        end_point = plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='lightcoral', markersize=15, label='End Node')\n",
        "        legend = plt.legend(handles=[regular_line, traversal_line, start_point, end_point], loc='upper left', bbox_to_anchor=(0, 1), ncol=2)\n",
        "        legend.get_frame().set_alpha(0.8)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def print_filtered_content(traversal_path, filtered_content):\n",
        "        \"\"\"\n",
        "        Prints the filtered content of visited nodes in the order of traversal.\n",
        "\n",
        "        Args:\n",
        "        - traversal_path (list of int): The list of node indices representing the traversal path.\n",
        "        - filtered_content (dict of int: str): A dictionary mapping node indices to their filtered content.\n",
        "\n",
        "        Returns:\n",
        "        - None\n",
        "        \"\"\"\n",
        "        print(\"\\nFiltered content of visited nodes in order of traversal:\")\n",
        "        for i, node in enumerate(traversal_path):\n",
        "            print(f\"\\nStep {i + 1} - Node {node}:\")\n",
        "            print(f\"Filtered Content: {filtered_content.get(node, 'No filtered content available')[:200]}...\")  # Print first 200 characters\n",
        "            print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the graph RAG class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GraphRAG:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the GraphRAG system with components for document processing, knowledge graph construction,\n",
        "        querying, and visualization.\n",
        "        \n",
        "        Attributes:\n",
        "        - llm: An instance of a large language model (LLM) for generating responses.\n",
        "        - embedding_model: An instance of an embedding model for document embeddings.\n",
        "        - document_processor: An instance of the DocumentProcessor class for processing documents.\n",
        "        - knowledge_graph: An instance of the KnowledgeGraph class for building and managing the knowledge graph.\n",
        "        - query_engine: An instance of the QueryEngine class for handling queries (initialized as None).\n",
        "        - visualizer: An instance of the Visualizer class for visualizing the knowledge graph traversal.\n",
        "        \"\"\"\n",
        "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
        "        self.embedding_model = OpenAIEmbeddings()\n",
        "        self.document_processor = DocumentProcessor()\n",
        "        self.knowledge_graph = KnowledgeGraph()\n",
        "        self.query_engine = None\n",
        "        self.visualizer = Visualizer()\n",
        "\n",
        "    def process_documents(self, documents):\n",
        "        \"\"\"\n",
        "        Processes a list of documents by splitting them into chunks, embedding them, and building a knowledge graph.\n",
        "        \n",
        "        Args:\n",
        "        - documents (list of str): A list of documents to be processed.\n",
        "        \n",
        "        Returns:\n",
        "        - None\n",
        "        \"\"\"\n",
        "        splits, vector_store = self.document_processor.process_documents(documents)\n",
        "        self.knowledge_graph.build_graph(splits, self.llm, self.embedding_model)\n",
        "        self.query_engine = QueryEngine(vector_store, self.knowledge_graph, self.llm)\n",
        "\n",
        "    def query(self, query: str):\n",
        "        \"\"\"\n",
        "        Handles a query by retrieving relevant information from the knowledge graph and visualizing the traversal path.\n",
        "        \n",
        "        Args:\n",
        "        - query (str): The query to be answered.\n",
        "        \n",
        "        Returns:\n",
        "        - str: The response to the query.\n",
        "        \"\"\"\n",
        "        response, traversal_path, filtered_content = self.query_engine.query(query)\n",
        "        \n",
        "        if traversal_path:\n",
        "            self.visualizer.visualize_traversal(self.knowledge_graph.graph, traversal_path)\n",
        "        else:\n",
        "            print(\"No traversal path to visualize.\")\n",
        "        \n",
        "        return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define documents path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path = \"data/Understanding_Climate_Change.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = PyPDFLoader(path)\n",
        "documents = loader.load()\n",
        "documents = documents[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a graph RAG instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph_rag = GraphRAG()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Process the documents and create the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph_rag.process_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Input a query and get the retrieved information from the graph RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"what is the main cause of climate change?\"\n",
        "response = graph_rag.query(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=all-rag-techniques--graph-rag)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
